<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Impact of Bayesian Attention Mechanism</title>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            line-height: 1.6;
            /* color: #333; */
            /* background-color: #181818; */
            background-image: url('image.png');

            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 1em;
            background-color: #fff;
            border-radius: 40px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        }
        h1, h2, h3 {
            border-bottom: 1px solid #eee;
            padding-bottom: 10px;
            margin-top: 1.5em;
        }
        a {
            color: #007bff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        /* code {
            background-color: #f0f0f0;
            padding: 2px 5px;
            border-radius: 4px;
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
        } */
        ul, ol {
            padding-left: 20px;
        }
        li {
            margin-bottom: 10px;
        }
        /* Style for the nested list item */
        ol > li > ul {
            margin-top: 10px;
        }

        .python{
            background-color: #181818;
            /* border-left: 4px solid #007bff; */
            color: #d4d4d4;
            padding: 1em;
            margin-top: -60px;
            margin-bottom: -40px;
            border-style: solid;
            border-width: 50px;
            border-radius: 80px;
            /* border-top-width: 10px; */
            border-color: white;
        }
    </style>
</head>
<body>


    <div class="container">
        <h1>Bayesian Attention Mechanism Impact</h1>
         <h5>By Arthur S. Bianchessi</h5>

        <p>Last week, I published <a href="https://arxiv.org/abs/2505.22842">"Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation"</a> on arXiv. The paper introduces the Bayesian Attention Mechanism Theoretical Framework that enhances the capabilities of transformer models enabling models to extrapolate context lengths. This article discusses the potential impact of this framework on the future of large language models, particularly in terms of compute savings and the implications for hardware investments.</p>
        </p>

        <h3>Resources</h3>
        <ul>
          <li><a href="https://arxiv.org/abs/2505.22842">Paper</a></li>
          <li><a href="https://github.com/ArthurSBianchessi/BAM">GitHub</a></li>
        </ul>

        <h3>Introduction</h3>

        <p>Current models have scaled a lot due to the increase in capability of investing more computation in model training. However, there a two quadratic costs that are associated in training transformer models:</p>
        <ol>
          <li>To train a model with twice the number of parameters, you also need twice the amount of data, therefore resulting in a 4x increase in computational cost.
            <ul>
              <li>By rule of thumb, the number of tokens in the training data should be around 20x the number of parameters in the model, for it to be the best performing model achievable with a given amount of compute. This idea of scaling laws was first introduced in <a href="https://arxiv.org/abs/2001.08361">"Scaling Laws for Neural Language Models"</a>, but the rule of thumb is taken from <a href="https://arxiv.org/abs/2203.15556">"Training Compute-Optimal Large Language Models"</a>.</li>
            </ul>
          </li>
          <li>The attention mechanism in transformers has a quadratic cost in the sequence length, which means that doubling the sequence length results in a 4x increase in computational cost.</li>
        </ol>

        <p>Altough current model sizes have stagnated, the sequence length has not. Companies such as OpenAI, Google, and Meta have invested a lot of resources in increasing the sequence length of their models, resulting in models that can handle millions of tokens. However, in the current literature, there is no way to extrapolate the context length of a model beyond the training data without any modification to the underlying model<sup>1</sup>. This means that if a model is trained on 128K tokens, it cannot handle more than 128K tokens at inference time.</p>

        <p>In our <a href="https://arxiv.org/abs/2505.22842">paper</a>, we managed to train a small model (~120M) in a sequence length of 512 tokens, and extrapolate the context length of more than 128K tokens<sup>2</sup>, beating the context length of models such as <a href="https://arxiv.org/abs/2407.21783">Llama-3</a>. To estimate the saving that would be achieved by using this method, I am going to use the <a href="https://arxiv.org/abs/2407.21783">Llama-3 herd of models</a> as reference.</p>

        <h3>Estimating Savings</h3>

        <p>Llama-3 main model <code>Llama-3.1-405B</code> was trained on around 15 Trillion tokens. These tokens were used in different stages of training, increasing the context length trained on at each stage, the specific scalings used in the training can be found at section 3.4 of <a href="https://arxiv.org/abs/2407.21783">Llama-3 herd of models</a>. For the sake of simplicity, I am considering:</p>
        <ul>
          <li>15 Trillion tokens at 8K context length<sup>3</sup></li>
          <li>100 Billion tokens at 128K context length<sup>4</sup></li>
        </ul>

        <p>Considering these variables, we can estimate the proportion of compute that is used in the training of the model at 128K context length following the formula:</p>

        <p>
        $$
        \text{Proportion} = \frac{\text{Compute(Long Context)}}{\text{Compute(8K)} + \text{Compute(Long Context)}}
        $$
        </p>

        <p>Where the compute is approximated as the number of tokens multiplied by the square of the context length. This means that we can calculate the proportion of compute used in training the model at 128K context length as follows:</p>

        <p>
        $$
        \frac{(1 \times 10^{11}) \times 128\ 000^2}{\Big((15 \times 10^{12}) \times 8\ 000^2\Big) + \Big((1 \times 10^{11}) \times 128\ 000^2\Big)} \approx 63.054\% 
        $$
        </p>

        <p>This means that around 63% of the compute used in training <code>Llama-3.1-405B</code> was used in the training of the model for 128K context length. This is a very rough estimate, but it gives us a lower bound for the savings that could be achieved. In this specific case, training the same model would require almost $3\times$ less compute. If we consider context lengths of 1M tokens, the savings are exponentially greater, as the proportion of compute used in training the model at 1M context length would be even lower. As it follows</p>

        <p>
        $$
        \frac{(1 \times 10^{11}) \times (10^6)^2}{\Big((15 \times 10^{12}) \times 8\ 000^2\Big) + \Big((1 \times 10^{11}) \times (10^6)^2\Big)} \approx 99.049\%
        $$
        </p>

        <p>in this case, the savings would be around $100\times$ less compute. Even though the neighter the number of tokens nor the context length were aproximated, this estimate is still an underestimation. This is due to the extrapolation of context length, which allows for an increase of $500\times$ context at $\text{~100%}$ accuracy in passkey retrieval. Considering this setup:
        </p>

        <p>
        $$
        \frac{(1 \times 10^{11}) \times (10^6)^2}{\Big((15 \times 10^{12}) \times 2\ 000^2\Big) + \Big((1 \times 10^{11}) \times (10^6)^2\Big)} \approx 99.94\%
        $$
        </p>

        <p>In this case, the savings would be around $1600\times$ less compute, which is an absurd reduction in compute and training cost.


        <h3>Conclusion</h3>
         <p>As shown, Bayesian Attention Mechanism allows for a significant reduction in the compute required to train models with large context lengths. Although the estimates done in this post are meant to be a lower bound, they shouldn't reflect the actual savings that can be achieved. This is due to the fact that companies such as Meta and OpenAI have invested a lot of resources in increasing their compute capabilities, and, as such, they are likely to continue to invest such savings in other improvements, such as training larger models. In other words, if the results of Bayesian Attention Mechanism are demonstrated to be consistent, this should result in a significant reduction in the cost of training models as well as an increse in the model capabilities.</p>


        <h3>Notes</h3>
        <p>1. There are some methods that allow for extrapolation of context length, that modify the underlying model, but do not need any additional training.  <a href="https://arxiv.org/abs/2501.19399">Scalable Softmax</a> is the one that has the best performance that I am aware of, being published in January 31st, 2025. It allows for around $8\times$ extrapolation of context length with minimal degradation in performance. <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">Llama-4 herd of models</a> are examples of models that use this method to extrapolate context length. Meta in this case seems to have used the method to train models from 128K to 1M tokens (10M in the case of Llama 4 Scout).</p>
        <p>2. We did train models with larger context lengths, the model trained on sequence length of 2048 managed to get 100% accuracy in the passkey retrieval (10 samples) of 1 Million tokens (this test was done after publishing the paper). Initial testing of larger models ($\text{~500M}$ parameters) also show that these models can continue to extrapolate more than 2 orders of magnitude in context length($>100\times$). </p>
        <p>3. It was used less than 15 Trillion tokens in this context length, but I rounded it up to 15 Trillion so it would underestimate the savings.</p>
        <!-- <p>4. 100B tokens trained in 128K context length is also an underestimation. Considering a sequence length that is exponentially increased at each batch (most samples are trained in lower sequence lengths), the equivalent sequence length trained on 100B tokens is around 153K tokens of sequence length. This value was calculated using the following python code: </p> -->
         <!-- Justify setting max sequence length at 100 Billion tokens, it also should be an underestimation -->
        <p>4. Acording to the <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">report</a>, Meta used 800B tokens divided in 6 stages for long context pre-training. Due to this, I believe that the 100B tokens trained in 128K context length is a good approximation, also being an underestimation. This also is corraborated by aproximating the sequence length trained on 100B tokens through exponentially increasing sequence lengths at each batch. The equivalent sequence length trained on 100B tokens is around 153K tokens of sequence length. This value was calculated using the following python code:</p>

<pre class="python">
    <span style="color: #9a6a96">import</span> <span style="color: #4dc9b0">torch</span>
    <span style="color: #9a6a96">import</span> <span style="color: #4dc9b0">math</span>

    <span style="color: #9bcadc">initial_sequence_length</span>      = <span style="color: #abc2a2">8_000</span>
    <span style="color: #9bcadc">final_sequence_length</span>        = <span style="color: #abc2a2">128_000</span>
    <span style="color: #9bcadc">total_number_of_tokens</span>       = <span style="color: #abc2a2">800_000_000_000</span>
    <span style="color: #9bcadc">tokens_per_batch</span>             = <span style="color: #abc2a2">16_000_000</span> 
    <span style="color: #9bcadc">number_of_batches</span>            = <span style="color: #9bcadc">total_number_of_tokens</span> // <span style="color: #9bcadc">tokens_per_batch</span> 
    <span style="color: #9bcadc">sequence_length_per_batch</span>    = <span style="color: #4dc9b0">torch</span><span style="color: #d3d3a3">.logspace</span><span style="color: #fdd402">(</span><span style="color: #4dc9b0">math</span><span style="color: #d3d3a3">.log</span><span style="color: #fdd402">(</span><span style="color: #9bcadc">final_sequence_length</span><span style="color: #fdd402">)</span>, 
                                                  <span style="color: #4dc9b0">math</span><span style="color: #d3d3a3">.log</span><span style="color: #fdd402">(</span><span style="color: #9bcadc">initial_sequence_length</span><span style="color: #fdd402">)</span>, 
                                                  <span style="color: #9bcadc">number_of_batches</span>+<span style="color: #abc2a2">1</span>, <span style="color: #9bcadc">base</span>=<span style="color: #4dc9b0">math</span>.<span style="color: #9bcadc">e</span>, 
                                                  <span style="color: #9bcadc">dtype</span>=<span style="color: #4dc9b0">torch</span>.<span style="color: #9bcadc">float64</span><span style="color: #fdd402">)</span><span style="color: #fdd402">[</span>:-<span style="color: #abc2a2">1</span><span style="color: #fdd402">]</span>
    <span style="color: #9bcadc">cost_per_batch</span>               = <span style="color: #9bcadc">tokens_per_batch</span> * <span style="color: #fdd402">(</span><span style="color: #9bcadc">sequence_length_per_batch</span>**<span style="color: #abc2a2">2</span><span style="color: #fdd402">)</span>
    <span style="color: #9bcadc">total_cost</span>                   = <span style="color: #9bcadc">cost_per_batch</span><span style="color: #d3d3a3">.sum</span><span style="color: #fdd402">()</span>
    <span style="color: #9bcadc">equivalent_seq_len</span>           = <span style="color: #fdd402">(</span><span style="color: #9bcadc">total_cost</span> / <span style="color: #abc2a2">100_000_000_000</span><span style="color: #fdd402">)</span>**<span style="color: #abc2a2">0.5</span>
    <span style="color: #d3d3a3">print</span><span style="color: #fdd402">(</span><span style="color: #cb8f76">"100B sequence length equivalent: "</span>, <span style="color: #9bcadc">end</span>=<span style="color: #cb8f76">"</span><span style="color: #cb8f76">\t</span><span style="color: #cb8f76">"</span><span style="color: #fdd402">)</span>
    <span style="color: #d3d3a3">print</span><span style="color: #fdd402">(</span><span style="color: #569cd6">f</span><span style="color: #cb8f76">"</span><span style="color: #da70d6">{</span><span style="color: #9bcadc">equivalent_seq_len</span><span style="color: #569cd6">:,.2f</span><span style="color: #da70d6">}</span><span style="color: #cb8f76"> tokens"</span><span style="color: #fdd402">)</span>
</pre>

        <p> Although the above code does not maintain $\text{equivalent_seq_len} > \text{final_sequence_length}$ for $\text{final_sequence_length} = 1 000 000$, it retains this characteristic if the number of tokens is scaled by $\frac{\log{(\frac{\text{new large context}}{8\ 000})}}{\log(\frac{128\ 000}{8\ 000})}$. This should be done for the proportional increase in context length at each step to be the same as it is when training for 128K context length. The intuition behind this is that it should take more steps to scale a model to 1M tokens than it takes to scale it to 128K tokens. Due to this, I used the 100B tokens trained in the final sequence length rule of thumb for other calculations.</p>


        <h3>Disclaimer</h3>
        <p>This post is not financial advice. I am not a financial advisor.</p>

        <p>Although the main reason for this post is to publicize the technology, I do have a financial interests with this post.</p>

        <p>Due to all the reasons above and that I am not an established researcher, I do <strong>strongly recommend to independently verify</strong> the results of the paper. I do believe that this paper will have significant impact in the stock market, but again, these results should be <strong>verified</strong>. The code in the <a href="https://arxiv.org/abs/2505.22842">GitHub repository</a> is designed to be easy to run, and to reproduce the results of the paper should take less than a day using 8 instances of NVIDIA A100 40GB (The code should also be checked, anyone can print 100% accuracy)</p>
        

</body>
</html>